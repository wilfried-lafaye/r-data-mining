---
title: "Bayesian Classification and Discriminant Factor Analysis"
subtitle: "Project 3: Analysis of French Doctoral Theses"
author: 
  - Marie Bouetel
  - Wilfried LAFAYE
date: "ESIEE Paris | 2025 - 2026"
output:
  html_document:
    theme: flatly
    highlight: tango
    code_folding: hide
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
    number_sections: true
    df_print: paged
---

<style type="text/css">

body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    font-size: 16px;
    line-height: 1.6;
    color: #333;
    text-align: justify;
}

h1, h2, h3 {
    color: #2c3e50;
    font-weight: 600;
}

h1 {
    margin-top: 40px;
    border-bottom: 3px solid #18bc9c;
    padding-bottom: 10px;
}

h2 {
    margin-top: 30px;
    border-left: 5px solid #18bc9c;
    padding-left: 10px;
}

.title {
    text-align: center;
    font-size: 2.5em;
    font-weight: bold;
    color: #2c3e50;
    margin-bottom: 10px;
}

.subtitle {
    text-align: center;
    font-size: 1.5em;
    color: #7f8c8d;
    margin-bottom: 30px;
}

.author, .date {
    text-align: center;
    font-size: 1.2em;
    color: #34495e;
}

.main-container {
    max-width: 1000px;
    margin-left: auto;
    margin-right: auto;
}

blockquote {
    background: #f9f9f9;
    border-left: 5px solid #18bc9c;
    margin: 1.5em 10px;
    padding: 0.5em 10px;
    font-style: italic;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center")
```

# Introduction

```{r load-libs, message=FALSE, warning=FALSE}
library(tidyverse)
library(tm)
library(MASS)
library(caret)
library(SnowballC)
library(topicmodels)
library(text2vec)
library(kernlab) # For SVM
library(klaR)    # For optimized Naive Bayes

```


The objective of this project is to implement an **advanced Bayesian classification** combined with **Discriminant Factor Analysis (AFD/LDA)** to categorize abstracts of French doctoral theses according to their field of study.

We will compare two vectorization approaches:

1. **Sparse Approach (TF-IDF)**: Classic vectorization based on term frequency.
2. **Word Embeddings Approach**: Dense word representation capturing semantics.

## The Dataset

We use the **French Doctoral Thesis Semantic Similarity Search** dataset available on Kaggle. This dataset contains information about French theses, including their abstracts, titles, and fields of study.

*   **Source**: [Kaggle - Antoine Bourgois](https://www.kaggle.com/code/antoinebourgois2/french-doctoral-thesis-semantic-similarity-search)
*   **Challenge**: The textual and semantic nature of the data makes classification complex.

## Objectives

1.  **NLP Preprocessing**: Cleaning, vectorization (TF-IDF/Embeddings).
2.  **Feature Extraction**: Topic Modeling, NLP features.
3.  **Dimensionality Reduction**: Using Linear Discriminant Analysis (LDA) to project the data.
4.  **Classification**: Training a Bayesian classifier (Naive or advanced).
5.  **Evaluation**: Performance analysis (Accuracy, F1-score, Confusion Matrices).

---

# Data Loading and Preparation

```{r download-data}

data_dir <- "../data"
dataset_file <- file.path(data_dir, "french_thesis_20231021_metadata.csv")
zip_file <- file.path(data_dir, "dataset.zip")


kaggle_json <- "~/.kaggle/kaggle.json"

if (!file.exists(dataset_file)) {
  cat(" Checking data...\n")
  
  if (!dir.exists(data_dir)) dir.create(data_dir, recursive = TRUE)
  

  if (file.exists(kaggle_json)) {

    json_content <- paste(readLines(kaggle_json, warn = FALSE), collapse = " ")
    username <- sub('.*"username"\\s*:\\s*"([^"]+)".*', "\\1", json_content)
    key <- sub('.*"key"\\s*:\\s*"([^"]+)".*', "\\1", json_content)
    
    cat(" Kaggle credentials detected (Curl).\n")
    

    url <- "https://www.kaggle.com/api/v1/datasets/download/antoinebourgois2/french-doctoral-thesis"
    

    cmd <- sprintf("curl -s -L -u %s:%s -o %s %s", 
                   username, key, zip_file, url)
    
    cat(" Downloading via Curl...\n")
    exit_code <- system(cmd)
    
    if (exit_code == 0 && file.exists(zip_file)) {
      cat(" Download successful! Unzipping...\n")
      unzip(zip_file, exdir = data_dir)
      file.remove(zip_file)
    } else {
      cat(" Curl download failed. Code:", exit_code, "\n")
    }
    
  } else {
    cat(" ~/.kaggle/kaggle.json file not found.\n")
    cat("For automatic download, place your Kaggle API key at this location.\n")
    cat("Otherwise, manually download the files into the 'data' folder.\n")
  }
} else {
  cat(" Data is already present.\n")
}


if (file.exists(dataset_file)) {

    df <- read_csv(dataset_file, show_col_types = FALSE)
    

    cat(" Initial dataset dimensions: ", paste(dim(df), collapse = " x "), "\n")
    

    df <- df %>% 
      filter(!is.na(Description) & Description != "") %>%
      distinct(Description, .keep_all = TRUE)
    
    cat(" Number of theses with usable abstract: ", nrow(df), "\n")
    

    set.seed(123)
    df <- df %>% sample_n(min(10000, nrow(df)))
}
```

---

# Data Preprocessing (Text Mining)

To transform thesis abstracts into data usable by our models, we will apply the following steps:

*   **Cleaning**: Removal of punctuation, conversion to lowercase.
*   **Stopwords**: Elimination of empty words (the, a, of, etc.) which do not provide discriminating semantic information.
*   **Stemming / Lemmatization**: Reducing words to their root to group similar terms.

## Cleaning Implementation

```{r cleaning}

clean_corpus <- function(corpus) {

  corpus <- tm_map(corpus, content_transformer(tolower))
  

  corpus <- tm_map(corpus, removePunctuation)
  

  corpus <- tm_map(corpus, removeNumbers)
  

  corpus <- tm_map(corpus, removeWords, stopwords("french"))
  

  corpus <- tm_map(corpus, stemDocument, language = "french")
  

  corpus <- tm_map(corpus, stripWhitespace)
  
  return(corpus)
}


if (exists("df")) {

  cat(" Creating corpus...\n")
  docs <- VCorpus(VectorSource(df$Description))
  

  cat("\n### Text Example BEFORE cleaning (Document 1):\n")
  raw_content <- as.character(docs[[1]])
  cat(str_wrap(raw_content, width = 80), "\n")
  

  cat("\n Applying cleaning...\n")
  docs <- clean_corpus(docs)
  

  cat("\n### Text Example AFTER cleaning (Document 1):\n")
  cleaned_content <- as.character(docs[[1]])
  cat(str_wrap(cleaned_content, width = 80), "\n")
}
```

---

# Visualizing Class Distribution

Before proceeding with classification, it is essential to understand the distribution of theses by field. As we are working on a sample, we want to identify the most represented themes.

```{r class_distribution}
if (exists("df")) {
  # Calculate counts per field
  domain_counts <- df %>%
    count(Domain, sort = TRUE) %>%
    mutate(Domain = fct_reorder(Domain, n))
  
  cat(" Total number of unique fields :", nrow(domain_counts), "\n")
  
  # Display the top 15 fields
  ggplot(head(domain_counts, 15), aes(x = Domain, y = n, fill = n)) +
    geom_col() +
    coord_flip() +
    scale_fill_gradient(low = "#a1d99b", high = "#18bc9c") +
    theme_minimal() +
    labs(title = "Top 15 Research Fields",
         subtitle = "Based on raw data sample",
         x = "Field of Study",
         y = "Number of Theses") +
    theme(legend.position = "none")
}
```

> **Note**: Due to the high dispersion of data (several hundred domains), we will limit our classification models to the **top 8 most frequent domains** to ensure sufficient statistical power for training.

---

# PART 1: Sparse TF-IDF + AFD Approach

This first approach uses **TF-IDF** vectorization (Term Frequency - Inverse Document Frequency) to represent documents as sparse vectors, and then applies **Discriminant Factor Analysis** for classification.

## TF-IDF Vectorization

```{r tfidf}
if (exists("docs")) {
  cat(" Creating Document-Term Matrices...\n")
  

  dtm_counts <- DocumentTermMatrix(docs)
  dtm_counts <- removeSparseTerms(dtm_counts, 0.99)
  

  rowTotals <- apply(dtm_counts , 1, sum)
  dtm_counts <- dtm_counts[rowTotals> 0, ]
  
  cat(" DTM dimensions (Counts) : ", paste(dim(dtm_counts), collapse = " x "), "\n")
  

  dtm_tfidf <- weightTfIdf(dtm_counts)
  cat(" DTM dimensions (TF-IDF) : ", paste(dim(dtm_tfidf), collapse = " x "), "\n")
}
```

## Topic Modeling (LDA)

We use the **Latent Dirichlet Allocation (LDA)** algorithm to extract hidden "topics" in the abstracts. The LDA model assumes that each document is a mixture of topics, and each topic is a mixture of words.

```{r lda_modeling}
k_topics <- 30

if (exists("dtm_counts")) {
  cat(" Training LDA model with k =", k_topics, "topics...\n")
  

  lda_model <- LDA(dtm_counts, k = k_topics, control = list(seed = 1234))
  

  cat("\n###  Top terms by Topic:\n")
  terms_lda <- terms(lda_model, 5)
  print(terms_lda)
  


  lda_features <- posterior(lda_model)$topics
  colnames(lda_features) <- paste0("Topic_", 1:k_topics)
  
  cat("\n LDA features dimensions: ", paste(dim(lda_features), collapse = " x "), "\n")
  head(lda_features)
}
```

These probabilities (`Topic_1` ... `Topic_k`) represent the contribution of each topic to the document and will be used as explanatory variables.

*   **Note**: We applied a **sparsity** threshold to avoid memory explosion and only keep significant terms.

## Data Alignment

For the following steps, we need to make sure that our features match the metadata (the `df` dataset) correctly, specifically the target variable `Domain`.

```{r align_data_sparse}
if (exists("lda_features") && exists("df")) {

  valid_indices <- as.numeric(rownames(lda_features))
  

  df_clean <- df[valid_indices, ]
  

  df_model_sparse <- cbind(df_clean, as.data.frame(lda_features))
  


  top_domains <- names(sort(table(df_model_sparse$Domain), decreasing = TRUE))[1:8]
  
  df_final_sparse <- df_model_sparse %>% 
    filter(Domain %in% top_domains) %>%
    mutate(Domain = as.factor(Domain))
    
  cat(" Filtered data for top 8 domains :", nrow(df_final_sparse), "theses.\n")
  cat(" Retained domains:\n")
  print(levels(df_final_sparse$Domain))
}
```

## Dimensionality Reduction (AFD / LDA)

We now use **Linear Discriminant Analysis** (LDA, from the `MASS` package) to project our features (topic probabilities) into a space that maximizes separation between fields.

```{r lda_reduction_sparse}
if (exists("df_final_sparse")) {

  formula_lda <- as.formula(paste("Domain ~", paste(colnames(lda_features), collapse = " + ")))
  lda_discrim_sparse <- lda(formula_lda, data = df_final_sparse)
  
  cat(" AFD Model trained.\n")
  

  lda_values_sparse <- predict(lda_discrim_sparse)$x
  

  df_plot_sparse <- data.frame(
    Domain = df_final_sparse$Domain,
    LD1 = lda_values_sparse[,1],
    LD2 = lda_values_sparse[,2]
  )
  

  library(ggplot2)
  
  p_sparse <- ggplot(df_plot_sparse, aes(x = LD1, y = LD2, color = Domain)) +
    geom_point(alpha = 0.6) +
    stat_ellipse(level = 0.95) +
    theme_minimal() +
    labs(title = "AFD Projection - Sparse TF-IDF Approach",
         subtitle = "Discriminant Axes LD1 vs LD2",
         x = "First Discriminant Axis (LD1)",
         y = "Second Discriminant Axis (LD2)") +
    theme(legend.position = "right")
    
  print(p_sparse)
}
```

## Bayesian Classification (Sparse)

### Train / Test Split

```{r split_data_sparse}
if (exists("df_final_sparse")) {
  set.seed(42)

  trainIndex_sparse <- createDataPartition(df_final_sparse$Domain, p = .8, 
                                    list = FALSE, 
                                    times = 1)
  
  df_train_sparse <- df_final_sparse[ trainIndex_sparse,]
  df_test_sparse  <- df_final_sparse[-trainIndex_sparse,]
  
  cat(" Training data :", nrow(df_train_sparse), "observations\n")
  cat(" Testing data  :", nrow(df_test_sparse), "observations\n")
}
```

### Model Optimization and Validation (Grid Search)

```{r model-optimization-sparse}
if (exists("df_train_sparse")) {


  fitControl <- trainControl(method = "repeatedcv", 
                             number = 10, 
                             repeats = 3)
  

  searchGrid <- expand.grid(
    usekernel = c(TRUE, FALSE),
    laplace = c(0, 0.5, 1), 
    adjust = c(0.75, 1, 1.25)
  )
  
  cat(" Launching Grid Search (10-fold CV x 3)...\n")
  
  features_cols_sparse <- grep("Topic_", names(df_train_sparse), value = TRUE)
  

  nb_tuned_sparse <- train(x = df_train_sparse[, features_cols_sparse],
                    y = df_train_sparse$Domain,
                    method = "naive_bayes",
                    trControl = fitControl,
                    tuneGrid = searchGrid)
  

  print(nb_tuned_sparse)
  

  cat("\n Visualizing hyperparameter impact:\n")
  plot(nb_tuned_sparse)
  

  cat("\n Best configuration: \n")
  print(nb_tuned_sparse$bestTune)
}
```

### Final Evaluation (Sparse)

```{r evaluation-sparse}
if (exists("nb_tuned_sparse")) {

  predictions_sparse <- predict(nb_tuned_sparse, newdata = df_test_sparse[, features_cols_sparse])
  

  conf_mat_sparse <- confusionMatrix(predictions_sparse, df_test_sparse$Domain)
  
  cat(" Overall Accuracy (Sparse Approach) :", round(conf_mat_sparse$overall["Accuracy"] * 100, 2), "%\n\n")
  

  print(conf_mat_sparse$table)
  

  cat("\n Detailed metrics per class:\n")
  print(conf_mat_sparse$byClass[, c("Sensitivity", "Specificity", "F1")])
}
```

### Visualization of Results (Sparse)

```{r vis-sparse}
if (exists("conf_mat_sparse")) {

  df_cm_sparse <- as.data.frame(conf_mat_sparse$table)
  colnames(df_cm_sparse) <- c("Prediction", "Reference", "Freq")
  

  df_cm_sparse <- df_cm_sparse %>%
    group_by(Reference) %>%
    mutate(Percentage = Freq / sum(Freq)) %>%
    ungroup()

  ggplot(df_cm_sparse, aes(x = Reference, y = Prediction, fill = Percentage)) +
    geom_tile() +
    scale_fill_gradient(low = "white", high = "#2c3e50") +
    geom_text(aes(label = Freq), color = ifelse(df_cm_sparse$Percentage > 0.5, "white", "black")) +
    theme_minimal() +
    labs(title = "Confusion Matrix - Sparse TF-IDF Approach",
         subtitle = "Rows: Predictions | Columns: Ground Truth",
         x = "Actual Domain",
         y = "Predicted Domain") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```

---

# PART 2: Word Embeddings Approach

This second approach uses **Word Embeddings** to capture the semantics of terms. Unlike the sparse TF-IDF representation, embeddings produce dense vectors of a fixed dimension.

## Data Preparation for Word2Vec

```{r prepare_embeddings}
if (exists("docs") && exists("df")) {
  

  texts_clean <- sapply(docs, as.character)
  

  tokens <- itoken(texts_clean, 
                   preprocessor = tolower, 
                   tokenizer = word_tokenizer,
                   progressbar = FALSE)
  

  vocab <- create_vocabulary(tokens)
  vocab <- prune_vocabulary(vocab, term_count_min = 5, doc_proportion_max = 0.7)
  
  cat(" Vocabulary size:", nrow(vocab), "terms\n")
  
  vectorizer <- vocab_vectorizer(vocab)
}
```

## Training Word2Vec Model (GloVe)

We use the **GloVe** (Global Vectors for Word Representation) algorithm to create our word embeddings.

```{r train_embeddings}
if (exists("vectorizer")) {

  tcm <- create_tcm(tokens, vectorizer, skip_grams_window = 5L)
  

  glove <- GlobalVectors$new(rank = 200, x_max = 10)
  
  cat(" Training GloVe model...\n")
  word_vectors <- glove$fit_transform(tcm, n_iter = 20, convergence_tol = 0.01, n_threads = 4)
  

  word_vectors <- word_vectors + t(glove$components)
  
  cat(" Embeddings dimensions:", paste(dim(word_vectors), collapse = " x "), "\n")
}
```

## Creating Document Vectors

To represent each document, we calculate the average of the embeddings of all words in the document.

```{r doc_embeddings}
if (exists("word_vectors") && exists("texts_clean")) {

  # For TF-IDF weighting, we use the previously created vectorizer
  tfidf_model <- TfIdf$new()
  dtm_emb <- create_dtm(tokens, vectorizer)
  dtm_tfidf_emb <- tfidf_model$fit_transform(dtm_emb)

  # TF-IDF weighted embedding function
  get_doc_embedding_weighted <- function(doc_id, dtm_tfidf, word_vectors) {
    # Get words and their TF-IDF weights from the document
    doc_weights <- dtm_tfidf[doc_id, ]
    words_indices <- which(doc_weights > 0)
    words <- names(words_indices)
    weights <- as.numeric(doc_weights[words_indices])
    
    # Filter for words present in Word2Vec
    valid_mask <- words %in% rownames(word_vectors)
    if (!any(valid_mask)) return(rep(0, ncol(word_vectors)))
    
    valid_words <- words[valid_mask]
    valid_weights <- weights[valid_mask]
    
    # Weighted Average
    vectors <- word_vectors[valid_words, , drop = FALSE]
    colSums(vectors * valid_weights) / sum(valid_weights)
  }
  
  cat(" Creating TF-IDF weighted document embeddings...\n")
  doc_embeddings <- t(sapply(1:nrow(dtm_tfidf_emb), function(i) {
    get_doc_embedding_weighted(i, dtm_tfidf_emb, word_vectors)
  }))
  rownames(doc_embeddings) <- NULL
  colnames(doc_embeddings) <- paste0("Emb_", 1:ncol(doc_embeddings))
  
  cat(" Document embeddings dimensions:", paste(dim(doc_embeddings), collapse = " x "), "\n")
}
```

## Data Alignment (Embeddings)

```{r align_data_embeddings}
if (exists("doc_embeddings") && exists("df")) {

  valid_indices_emb <- which(rowSums(doc_embeddings) != 0)
  
  df_clean_emb <- df[valid_indices_emb, ]
  doc_embeddings_valid <- doc_embeddings[valid_indices_emb, ]
  

  df_model_emb <- cbind(df_clean_emb, as.data.frame(doc_embeddings_valid))
  

  top_domains_emb <- names(sort(table(df_model_emb$Domain), decreasing = TRUE))[1:8]
  
  df_final_emb <- df_model_emb %>% 
    filter(Domain %in% top_domains_emb) %>%
    mutate(Domain = as.factor(Domain))
    
  cat(" Filtered data for top 8 domains:", nrow(df_final_emb), "theses.\n")
}
```

## Dimensionality Reduction (AFD / LDA) with Embeddings

```{r lda_reduction_embeddings}
if (exists("df_final_emb")) {

  features_emb <- grep("Emb_", names(df_final_emb), value = TRUE)
  

  formula_lda_emb <- as.formula(paste("Domain ~", paste(features_emb, collapse = " + ")))
  lda_discrim_emb <- lda(formula_lda_emb, data = df_final_emb)
  
  cat(" AFD model trained on embeddings.\n")
  

  lda_values_emb <- predict(lda_discrim_emb)$x
  

  df_plot_emb <- data.frame(
    Domain = df_final_emb$Domain,
    LD1 = lda_values_emb[,1],
    LD2 = lda_values_emb[,2]
  )
  
  p_emb <- ggplot(df_plot_emb, aes(x = LD1, y = LD2, color = Domain)) +
    geom_point(alpha = 0.6) +
    stat_ellipse(level = 0.95) +
    theme_minimal() +
    labs(title = "AFD Projection - Word Embeddings Approach",
         subtitle = "Discriminant Axes LD1 vs LD2",
         x = "First Discriminant Axis (LD1)",
         y = "Second Discriminant Axis (LD2)") +
    theme(legend.position = "right")
    
  print(p_emb)
}
```

## Bayesian Classification (Embeddings)

### Train / Test Split

```{r split_data_embeddings}
if (exists("df_final_emb")) {
  set.seed(42)

  trainIndex_emb <- createDataPartition(df_final_emb$Domain, p = .8, 
                                    list = FALSE, 
                                    times = 1)
  
  df_train_emb <- df_final_emb[ trainIndex_emb,]
  df_test_emb  <- df_final_emb[-trainIndex_emb,]
  
  cat(" Training data :", nrow(df_train_emb), "observations\n")
  cat(" Testing data  :", nrow(df_test_emb), "observations\n")
}
```

### Model Optimization and Validation

```{r model-optimization-embeddings}
if (exists("df_train_emb")) {

  # AFD projection for classification (keeping more dimensions than for the plot)
  lda_values_all_emb <- predict(lda_discrim_emb)$x
  df_final_emb_lda <- cbind(df_final_emb %>% dplyr::select(-starts_with("Emb_")), as.data.frame(lda_values_all_emb))
  
  # Train/Test Split on AFD components
  set.seed(42)
  trainIndex_emb <- createDataPartition(df_final_emb_lda$Domain, p = .8, list = FALSE)
  df_train_emb_lda <- df_final_emb_lda[ trainIndex_emb,]
  df_test_emb_lda  <- df_final_emb_lda[-trainIndex_emb,]
  
  features_lda_emb <- grep("LD", names(df_train_emb_lda), value = TRUE)

  fitControl_emb <- trainControl(method = "cv", number = 5) # Faster for SVM
  
  cat(" Launching Grid Search for Naive Bayes on AFD...\n")
  nb_tuned_emb <- train(x = df_train_emb_lda[, features_lda_emb],
                    y = df_train_emb_lda$Domain,
                    method = "naive_bayes",
                    trControl = fitControl_emb)
                    
  cat(" Training an SVM (Radial) on AFD components...\n")
  svm_tuned_emb <- train(x = df_train_emb_lda[, features_lda_emb],
                        y = df_train_emb_lda$Domain,
                        method = "svmRadial",
                        trControl = fitControl_emb,
                        tuneLength = 5)
  
  print(svm_tuned_emb)
  
  cat("\n Best SVM configuration: \n")
  print(svm_tuned_emb$bestTune)
}
```

### Final Evaluation (Embeddings)

```{r evaluation-embeddings}
if (exists("svm_tuned_emb")) {
  # Comparing Naive Bayes vs SVM on tests
  pred_nb_emb <- predict(nb_tuned_emb, newdata = df_test_emb_lda[, features_lda_emb])
  pred_svm_emb <- predict(svm_tuned_emb, newdata = df_test_emb_lda[, features_lda_emb])
  
  conf_mat_nb_emb <- confusionMatrix(pred_nb_emb, df_test_emb_lda$Domain)
  conf_mat_emb <- confusionMatrix(pred_svm_emb, df_test_emb_lda$Domain) # Keeping SVM for the rest
  
  cat(" Naive Bayes Accuracy (AFD + Embeddings):", round(conf_mat_nb_emb$overall["Accuracy"] * 100, 2), "%\n")
  cat(" SVM Accuracy (AFD + Embeddings)        :", round(conf_mat_emb$overall["Accuracy"] * 100, 2), "%\n\n")
  
  print(conf_mat_emb$table)
  
  cat("\n Detailed SVM metrics per class:\n")
  print(conf_mat_emb$byClass[, c("Sensitivity", "Specificity", "F1")])
  
  cat("\n### Overfitting Analysis (Train vs Test)\n")

  # Predictions on TRAINING set
  pred_train_nb <- predict(nb_tuned_emb, newdata = df_train_emb_lda[, features_lda_emb])
  pred_train_svm <- predict(svm_tuned_emb, newdata = df_train_emb_lda[, features_lda_emb])

  # Train Confusion Matrices
  cm_train_nb <- confusionMatrix(pred_train_nb, df_train_emb_lda$Domain)
  cm_train_svm <- confusionMatrix(pred_train_svm, df_train_emb_lda$Domain)

  # Retrieving Test Accuracies
  acc_test_nb <- conf_mat_nb_emb$overall["Accuracy"]
  acc_test_svm <- conf_mat_emb$overall["Accuracy"]

  # Train Accuracy
  acc_train_nb <- cm_train_nb$overall["Accuracy"]
  acc_train_svm <- cm_train_svm$overall["Accuracy"]

  # Comparative table
  overfit_df <- data.frame(
    Nom_Modele = c("Naive Bayes (AFD)", "SVM (AFD)"),
    Train_Accuracy = c(acc_train_nb, acc_train_svm),
    Test_Accuracy = c(acc_test_nb, acc_test_svm)
  )

  overfit_df$Gap <- overfit_df$Train_Accuracy - overfit_df$Test_Accuracy

  # Formatted display
  overfit_df$Train_Accuracy <- scales::percent(overfit_df$Train_Accuracy, accuracy = 0.01)
  overfit_df$Test_Accuracy <- scales::percent(overfit_df$Test_Accuracy, accuracy = 0.01)
  overfit_df$Gap <- scales::percent(overfit_df$Gap, accuracy = 0.01)

  print(overfit_df)

  if (any(as.numeric(sub("%", "", overfit_df$Gap)) > 10)) {
     cat("\n WARNING: There is a risk of overfitting (gap > 10%).\n")
  } else {
     cat("\n No major overfitting detected (gap < 10%).\n")
  }
}
```

### Visualization of Results (Embeddings)

```{r vis-embeddings}
if (exists("conf_mat_emb")) {

  df_cm_emb <- as.data.frame(conf_mat_emb$table)
  colnames(df_cm_emb) <- c("Prediction", "Reference", "Freq")
  
  df_cm_emb <- df_cm_emb %>%
    group_by(Reference) %>%
    mutate(Percentage = Freq / sum(Freq)) %>%
    ungroup()

  ggplot(df_cm_emb, aes(x = Reference, y = Prediction, fill = Percentage)) +
    geom_tile() +
    scale_fill_gradient(low = "white", high = "#e74c3c") +
    geom_text(aes(label = Freq), color = ifelse(df_cm_emb$Percentage > 0.5, "white", "black")) +
    theme_minimal() +
    labs(title = "Confusion Matrix - Word Embeddings Approach",
         subtitle = "Rows: Predictions | Columns: Ground Truth",
         x = "Actual Domain",
         y = "Predicted Domain") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```

---

# Approaches Comparison

```{r comparison}
if (exists("conf_mat_sparse") && exists("conf_mat_emb")) {
  
  comparison_df <- data.frame(
    Approach = c("Sparse TF-IDF (NB)", "Word Embeddings (NB)", "Word Embeddings (SVM + AFD)"),
    Accuracy = c(
      round(conf_mat_sparse$overall["Accuracy"] * 100, 2),
      round(conf_mat_nb_emb$overall["Accuracy"] * 100, 2),
      round(conf_mat_emb$overall["Accuracy"] * 100, 2)
    ),
    Kappa = c(
      round(conf_mat_sparse$overall["Kappa"], 4),
      round(conf_mat_nb_emb$overall["Kappa"], 4),
      round(conf_mat_emb$overall["Kappa"], 4)
    )
  )
  
  cat(" Performance Comparison:\n")
  print(comparison_df)
  
  f1_sparse <- mean(conf_mat_sparse$byClass[, "F1"], na.rm = TRUE)
  f1_emb <- mean(conf_mat_emb$byClass[, "F1"], na.rm = TRUE)
  
  cat("\n Average F1-Score (Sparse) :", round(f1_sparse, 4))
  cat("\n Average F1-Score (Embeddings SVM) :", round(f1_emb, 4), "\n")
  
  comparison_plot <- data.frame(
    Approach = rep(c("Sparse", "Embed NB", "Embed SVM"), 2),
    Metric = c(rep("Accuracy", 3), rep("Avg F1", 3)),
    Score = c(comparison_df$Accuracy[1]/100, comparison_df$Accuracy[2]/100, comparison_df$Accuracy[3]/100, 
             f1_sparse, mean(conf_mat_nb_emb$byClass[, "F1"], na.rm = TRUE), f1_emb)
  )
  
  ggplot(comparison_plot, aes(x = Approach, y = Score, fill = Metric)) +
    geom_col(position = "dodge") +
    theme_minimal() +
    labs(title = "Vectorization Approaches and Models Comparison",
         y = "Score",
         x = "") +
    scale_y_continuous(labels = scales::percent_format()) +
    geom_text(aes(label = round(Score, 3)), position = position_dodge(width = 0.9), vjust = -0.5)
}
```

---

# Discussion

## Analysis of Approaches

### Sparse TF-IDF Approach
*   **Advantages**: Interpretable, memory efficient for sparse data.
*   **Limitations**: Does not capture word semantics (synonyms treated differently).

### Word Embeddings Approach
*   **Advantages**: Captures semantic relationships between words.
*   **Limitations**: Less interpretable, requires more data for good training.

## Analysis of Discriminant Factors

*   Fields with a high F1-score are those that have highly specific vocabulary (e.g., "Medicine" or "Chemistry").
*   Fields with a low score are likely those that share a lot of transversal vocabulary (e.g., "Social Sciences" vs "Humanities").

## Limitations

*   Handling class imbalance (if certain disciplines are overrepresented).
*   Embedding quality depends on the training corpus size.

---

# Conclusion

Summary of main results obtained:

*   **The Sparse TF-IDF approach** with Topic Modeling offers good interpretability of results.
*   **The Word Embeddings approach** seems to be a better choice to train a model than AFD + TF-IDF, allowing it to capture more subtle semantic relationships.

Improvement Perspectives:
*   Using Deep Learning models like BERT for better contextual representation.
*   Data augmentation techniques for minority classes.
*   Exploring ensemble methods combining both approaches.
