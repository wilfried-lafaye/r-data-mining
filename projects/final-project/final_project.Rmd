---
title: "AI Text Detection"
subtitle: "Final Project: Kaggle Challenge (LLM-Detect)"
author: 
  - Wilfried LAFAYE
  - Marie BOUËTEL
date: "ESIEE Paris | 2025 - 2026"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: yes
    highlight: tango
    code_folding: hide
    df_print: paged
---

<style type="text/css">
/* Custom CSS for a professional look */
body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    font-size: 16px;
    line-height: 1.6;
    color: #333;
    text-align: justify;
}

h1, h2, h3 {
    color: #2c3e50; /* Dark Blue */
    font-weight: 600;
}

h1 {
    margin-top: 40px;
    border-bottom: 3px solid #18bc9c; /* Teal accent from Flatly theme */
    padding-bottom: 10px;
}

h2 {
    margin-top: 30px;
    border-left: 5px solid #18bc9c;
    padding-left: 10px;
}

.title {
    text-align: center;
    font-size: 2.5em;
    font-weight: bold;
    color: #2c3e50;
    margin-bottom: 10px;
}

.subtitle {
    text-align: center;
    font-size: 1.5em;
    color: #7f8c8d;
    margin-bottom: 30px;
}

.author, .date {
    text-align: center;
    font-size: 1.2em;
    color: #34495e;
}

.main-container {
    max-width: 1000px;
    margin-left: auto;
    margin-right: auto;
}

blockquote {
    background: #f9f9f9;
    border-left: 5px solid #18bc9c;
    margin: 1.5em 10px;
    padding: 0.5em 10px;
    font-style: italic;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = NA
)
```

# Preamble

In the framework of this final project, we address the challenge of distinguishing between human-generated text and that produced by Artificial Intelligence (LLM). The goal is to develop a sophisticated classification system combining Discriminant Factor Analysis (DFA) for dimensionality reduction and Bayesian methods for classification.

## Context of AI-Generated Text Detection

The dataset comes from the "LLM-Detect" project on Kaggle. The proliferation of Large Language Models makes the ability to identify the origin of a text crucial, whether for academic, journalistic, or security reasons.

## Objectives of Using Bayesian Classification and DFA

1.  **Dimensionalize Reduction**: Use DFA to extract the most discriminant factors among numerous linguistic features.
2.  **Robust Classification**: Apply a Bayesian classifier (possibly with advanced techniques like MCMC) on the reduced space to obtain reliable probabilistic predictions.

# Data and Preprocessing

## Loading Data

In this section, we introduce the data used for our study. The challenge of AI-generated text detection suffers from an initial lack of real data. To mitigate this, several versions of enriched datasets have been proposed by the community. We load different available versions here to illustrate their evolution.

```{r load-datasets}
# Necessary libraries
library(tidyverse)
library(knitr)
library(kableExtra)

# File paths
path_raw <- "../datasets/llm-detect-ai-generated-text/train_essays.csv"
paths_drcat <- paste0("../datasets/train_drcat_0", 1:4, ".csv")

# Function to get dataset statistics
get_stats <- function(path, version_name) {
  df <- read.csv(path)
  # Handling different column names between raw and drcat
  label_col <- if("generated" %in% names(df)) "generated" else "label"
  source_col <- if("source" %in% names(df)) "source" else NULL
  
  nombre_sources_val <- if(!is.null(source_col)) n_distinct(df[[source_col]]) else "N/A"
  
  data.frame(
    Version = version_name,
    Total_Rows = nrow(df),
    Human_Texts = sum(df[[label_col]] == 0),
    AI_Texts = sum(df[[label_col]] == 1),
    Number_of_sources = as.character(nombre_sources_val)
  )
}

# Statistics compilation
stats_list <- list()
stats_list[[1]] <- get_stats(path_raw, "Original (Kaggle)")
for(i in 1:4) {
  stats_list[[i+1]] <- get_stats(paths_drcat[i], paste0("v0", i))
}

comparison_table <- bind_rows(stats_list)

# Display comparative table
kable(comparison_table, col.names = c("Version", "Total rows", "Human Texts (0)", "AI Texts (1)", "Number of sources"), 
      caption = "Evolution and comparison of available datasets") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

As we can see, the original dataset is extremely unbalanced (only 3 AI examples). The `train_drcat` versions progressively increase the diversity and quantity of AI-generated texts. We will use version **v04** for the rest of our study as it offers the best representativeness.

## Exploratory Analysis and Cleaning

We now focus on the selected dataset (**v04**). Let's start by examining its internal structure and the first few rows of data.

```{r load-v04}
# Loading the final dataset for analysis
train_data <- read.csv("../datasets/train_drcat_04.csv")

# NOTE: On a machine with 32 GB of RAM, we use the complete dataset.
# No downsampling necessary.
```

### Dataset Structure (v04)

The dataset is composed of the following columns:

```{r structure-check}
# Overview of the technical structure (readable table)
structure_tbl <- tibble::tibble(
  Column = names(train_data),
  Type = vapply(train_data, function(x) class(x)[1], character(1)),
  Missing_Values = vapply(train_data, function(x) sum(is.na(x)), integer(1)),
  Example = vapply(train_data, function(x) {
    val <- x[which(!is.na(x))[1]]
    if (is.null(val)) NA_character_ else as.character(val)
  }, character(1))
)

structure_tbl %>%
  kable(
    caption = "Dataset structure (types and missing values)",
    align = "l"
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```

The column details are as follows:

*   **`essay_id`**: Unique identifier for each textual essay.
*   **`text`**: The full content of the text (Human or AI).
*   **`label`**: The target variable (0 = Human, 1 = AI).
*   **`source`**: The origin of the text (specific language model or source dataset).
*   **`prompt`**: The subject or instruction used for generation.
*   **`fold`**: Variable for cross-validation.

This dataset presents a healthy balance (29,792 humans vs 14,414 AI), which will allow us to train our Bayesian classifier robustly without systematically favoring the majority class.

### Quality Verification

Before proceeding with cleaning, we perform a Data Quality Check to identify potential biases or errors.

```{r quality-check}
# 1. Verification of missing values
colSums(is.na(train_data))

# 2. Verification of textual duplicates
n_text_dups <- sum(duplicated(train_data$text))
cat("Number of exact textual duplicates:", n_text_dups, "\n")

# 3. Analysis of unique IDs
n_unique_ids <- n_distinct(train_data$essay_id)
cat("Number of unique essay IDs:", n_unique_ids, " out of ", nrow(train_data), " rows.\n")
```

**Quality observations:**

1.  **Reliability**: The analysis shows that there are **no missing values** in critical columns (`text`, `label`). Only the `prompt` column is partially filled (31,295 NA), which is normal given the diversity of sources.
2.  **Duplicates**: We identify **`r n_text_dups` exact textual duplicates**. Although this rate is extremely low (<0.2%), these rows will be removed during cleaning to ensure statistical rigor.
3.  **Consistency**: The labels are **100% consistent**. There are no cases where an identical text is assigned to both a human and an AI, attesting to the cleanliness of the source dataset.
4.  **Data Leakage Alert**: The major point of vigilance lies in the uniqueness of the IDs. With **`r n_unique_ids` unique IDs** for `r nrow(train_data)` rows, it's clear that some source essays were used to generate several variants via different LLMs. 
    *   *Consequence*: During the evaluation phase, we must perform a Train/Test split based on `essay_id` and not on individual rows, to prevent the model from "memorizing" variants of a text it has already seen in the training set.

## Text Cleaning

To guarantee the quality of our model, we begin by removing the previously identified textual duplicates, then apply a series of transformations to normalize the text.

```{r cleaning}
# 1. Removal of textual duplicates
train_data <- train_data[!duplicated(train_data$text), ]

# 2. Cleaning and normalization
# We use the 'tm' library for basic transformations
library(tm)

clean_text <- function(text_vector) {
  # Transformation to corpus to facilitate 'tm' transformations
  corpus <- VCorpus(VectorSource(text_vector))
  
  corpus <- corpus %>%
    tm_map(content_transformer(tolower)) %>%           # Lowercase
    tm_map(removePunctuation) %>%                      # Punctuation removal
    tm_map(removeNumbers) %>%                          # Numbers removal
    tm_map(removeWords, stopwords("english")) %>%      # English stopword removal
    tm_map(stripWhitespace)                            # Whitespace removal
    
  return(sapply(corpus, content_transformer(identity)))
}

# Note: Cleaning can be long for 44k rows.
# For this demonstration, we show the logic.
# In a real approach, we would parallelize or use 'quanteda' for performance.

cat("Number of rows:", nrow(train_data), "\n")
cat("Balance - Human Texts (0):", sum(train_data$label == 0), "\n")
cat("Balance - AI Texts (1):", sum(train_data$label == 1), "\n")
```

## Feature Extraction

According to the recommendations from our research (see `models.md`), we use an optimized TF-IDF vectorization.

### TF-IDF Vectorization

We use n-grams of size 3 to 5 and a minimum frequency threshold (`min_df = 2`) to capture complex stylistic patterns that often betray the use of an LLM.

```{r tf-idf}
library(quanteda)

# Creating a quanteda corpus (more performant for n-grams)
corp <- corpus(train_data, text_field = "text")
# Creating the Document-Feature Matrix (DFM) with n-grams 3 to 5
# Note: This step is memory-intensive but possible with 32 GB of RAM.

toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_ngrams(n = 3:5)
dfm_train <- dfm(toks) %>% dfm_trim(min_termfreq = 25)

# Display vectorization results
dfm_density <- Matrix::nnzero(dfm_train) / (as.numeric(ndoc(dfm_train)) * as.numeric(nfeat(dfm_train)))
cat("Matrix density:", round(dfm_density * 100, 4), "%\n")

```

The DFM (Document-Feature Matrix) creates a vector representation where each row is a text and each column represents an n-gram (sequence of 3, 4, or 5 words). The cells contain the number of occurrences of each n-gram in the text.
This matrix is very **sparse** because most texts do not contain all possible n-grams. This representation allows machine learning models to capture the stylistic patterns that differentiate human texts from AI-generated texts.

# Methodology and Experimental Protocol

## Validation Protocol

To evaluate our models rigorously and avoid the "Data Leakage" identified during the quality analysis (multiple rows for the same `essay_id`), we perform a Train/Test split based on essay identifiers and not on rows.

```{r splitting}
set.seed(123)
library(caret)

# 1. Split Creation (80% Train, 20% Test) based on unique essay_id
unique_ids <- unique(train_data$essay_id)
train_ids <- sample(unique_ids, length(unique_ids) * 0.8)

# 2. Dataframe separation
df_train <- train_data[train_data$essay_id %in% train_ids, ]
df_test <- train_data[!train_data$essay_id %in% train_ids, ]

# 3. TF-IDF Matrix Preparation (DFM)
corp_train <- corpus(df_train, text_field = "text")
corp_test <- corpus(df_test, text_field = "text")

# Full tokenization (n-grams 3 to 5) for machine with 32 GB RAM
toks_train <- tokens(corp_train, remove_punct = TRUE, remove_numbers = TRUE) %>% 
  tokens_ngrams(n = 3:5)

dfm_train <- dfm(toks_train) %>% 
  dfm_trim(min_termfreq = 10)

dfm_test <- tokens(corp_test, remove_punct = TRUE, remove_numbers = TRUE) %>% 
  tokens_ngrams(n = 3:5) %>% 
  dfm() %>% 
  dfm_match(featnames(dfm_train))

# Conversion for model compatibility (NaiveBayes, LiblineaR)
library(Matrix)
train_matrix <- as(dfm_train, "dgCMatrix")
test_matrix <- as(dfm_test, "dgCMatrix")

# Labels
y_train <- as.factor(df_train$label)
y_test <- as.factor(df_test$label)
```

### Train/Test Split Summary

```{r split-summary}
cat("Dimensions TF-IDF TRAIN:", nrow(dfm_train), "documents ×", ncol(dfm_train), "features\n")
cat("Dimensions TF-IDF TEST:", nrow(dfm_test), "documents ×", ncol(dfm_test), "features\n")
```

Note that the split is based on `essay_id` (not rows) to prevent data leakage. Each source essay is assigned to only one partition (Train or Test), preventing the model from being trained on variants of the same text it must test.

Before training, we specify the chosen DFA framework because it conditions the dimensionality reduction used by the classifier.

## Discriminant Factor Analysis (DFA)

DFA (or Linear Discriminant Analysis - LDA) seeks to project the data onto a subspace that maximizes the separation between classes. However, it is unable to handle our high-dimensional data directly (vocabulary > number of observations, collinearity).

## Hybrid Approach: LSA + DFA

To circumvent this problem, we adopt a two-step strategy:

1.  **Unsupervised Dimensionality Reduction (LSA)**: We use Selective Value Decomposition (SVD) to reduce the sparse TF-IDF matrix to 50 dense latent dimensions.

2.  **Supervised Classification (DFA)**: We apply DFA on these 50 components to find the optimal discriminant axis.

This approach allows us to visualize the data in the discriminant space and obtain a high-performance linear classifier.

### Model Training

We now train the two main models on the training set and generate predictions on the test set.

```{r training-evaluation}
# Library for multinomial Naive Bayes
library(naivebayes)

# --- 1. Naive Bayes ---
model_nb <- multinomial_naive_bayes(x = train_matrix, y = y_train)
pred_nb_class <- predict(model_nb, newdata = test_matrix)
pred_nb_prob <- predict(model_nb, newdata = test_matrix, type = "prob")[, "1"]
cm_nb <- confusionMatrix(pred_nb_class, y_test, mode = "prec_recall")

# --- 2b. DFA (LSA + LDA) ---
library(irlba)
library(MASS)

# 1. SVD on training matrix (SPARSE)
# Explicit conversion to dgCMatrix just in case
train_matrix_sparse <- as(dfm_train, "dgCMatrix")
# Truncated SVD at 50 dimensions
svd_model <- irlba(train_matrix_sparse, nv = 50)

# Train Projection
lsa_train <- as.data.frame(as.matrix(svd_model$u %*% diag(svd_model$d)))
# IMPORTANT: ensure row order is respected (usually yes with irlba on input matrix)
lsa_train$label <- y_train 

# 2. LDA Training
model_afd <- lda(label ~ ., data = lsa_train)

# 3. Test Projection
test_matrix_sparse <- as(dfm_test, "dgCMatrix")
# Projection: Test_LSA = Test_Matrix * V
# Note: irlba returns V (right singular vectors).
# The correct projection in the document space is T * V.
lsa_test <- as.data.frame(as.matrix(test_matrix_sparse %*% svd_model$v))

# 4. Prediction
pred_afd <- predict(model_afd, newdata = lsa_test)
pred_afd_class <- pred_afd$class
pred_afd_prob <- pred_afd$posterior[, "1"]

cm_afd <- confusionMatrix(pred_afd_class, y_test, mode = "prec_recall")

# 5. Visualization of the discriminant axis (LD1)
# Recover scores from the first axis (LD1) for the test set
ld_scores <- as.data.frame(pred_afd$x)
ld_scores$label <- y_test

ggplot(ld_scores, aes(x = LD1, fill = label)) +
  geom_density(alpha = 0.5) +
  scale_fill_manual(values = c("#18bc9c", "#e74c3c"), labels = c("Human", "AI")) +
  labs(title = "Distribution of the First Discriminant Axis (DFA)",
       subtitle = "Class separation in reduced LSA space",
       x = "LD1 axis score", y = "Density", fill = "Class") +
  theme_minimal()

```

The projection of the test data on the first discriminant axis (LD1), obtained after LSA reduction (50 dimensions) then application of the DFA, shows a global differentiation between human texts and AI-generated texts. Negative scores are mostly associated with human texts while positive scores correspond to AI texts.
An overlap persists around the central zone indicating that the separation is not perfectly linear but remains significant.

# Results and Evaluation

## Comparative Performance

Comparison of the tested models' performance including the "Ensemble" approach.

```{r metrics}
extract_metrics <- function(cm, model_name) {
  data.frame(
    Model = model_name,
    Accuracy = cm$overall["Accuracy"],
    Precision = cm$byClass["Precision"],
    Recall = cm$byClass["Recall"],
    F1_Score = cm$byClass["F1"]
  )
}

metrics_nb <- extract_metrics(cm_nb, "Naive Bayes")
metrics_afd <- extract_metrics(cm_afd, "DFA (on LSA 50 dim)")
metrics_all <- rbind(metrics_nb, metrics_afd)

kable(metrics_all, digits = 4, caption = "Compared performance on the test set") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

### Interpretation of Metrics

- **Accuracy**: Overall proportion of correct predictions (TP+TN)/(TP+TN+FP+FN)
- **Precision**: Among the texts classified as "AI", what proportion actually is? TP/(TP+FP)
- **Recall**: Among all the "AI" texts in the dataset, what proportion was correctly detected? TP/(TP+FN)
- **F1-Score**: Harmonic mean between Precision and Recall (balanced synthesis)

In a context of AI-generated text detection, high recall is particularly important to limit false negatives (undetected AI texts). The compromise observed here shows that dimensionality reduction via LSA + DFA simplifies the data space but entails a slight loss of discriminant information compared to the Naive Bayes model applied directly to full representations.

## Confusion Matrices

#### Naive Bayes Model

```{r cm-nb-visual}
# Display Naive Bayes confusion matrix
kable(
  cm_nb$table,
  caption = "Confusion Matrix - Naive Bayes",
  align = "c"
) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)

cat("True Positives (AI correctly detected):", cm_nb$table[2,2], "\n")
cat("False Positives (Human classified as AI):", cm_nb$table[1,2], "\n")
cat("True Negatives (Human correctly detected):", cm_nb$table[1,1], "\n")
cat("False Negatives (AI classified as human):", cm_nb$table[2,1], "\n")
```

The Naive Bayes model presents excellent performance with only 75 false positives and 32 false negatives, reflecting a very high ability to distinguish human texts from AI-generated texts with very few classification errors.

#### DFA Model (LSA + LDA)

```{r cm-afd-visual}
# Display DFA confusion matrix
kable(
  cm_afd$table,
  caption = "Confusion Matrix - DFA (LSA 50 dim)",
  align = "c"
) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)

cat("True Positives (AI correctly detected):", cm_afd$table[2,2], "\n")
cat("False Positives (Human classified as AI):", cm_afd$table[1,2], "\n")
cat("True Negatives (Human correctly detected):", cm_afd$table[1,1], "\n")
cat("False Negatives (AI classified as Human):", cm_afd$table[2,1], "\n")
```

The model based on DFA after LSA reduction commits more errors, with a significant number of false positives (797), indicating a tendency to over-detect AI texts although the number of false negatives remains relatively limited (66).

## Comparative Summary
```{r summary-table}

comparison <- data.frame(
  Aspect = c(
    "True positives (AI detected)",
    "True negatives (human detected)",
    "Detection rate (recall)",
    "Confidence (precision)",
    "Overall balance (F1)"
  ),
  NaiveBayes = c(
    cm_nb$table[2,2],
    cm_nb$table[1,1],
    round(cm_nb$byClass["Recall"], 3),
    round(cm_nb$byClass["Precision"], 3),
    round(cm_nb$byClass["F1"], 3)
  ),
  DFA_LSA = c(
    cm_afd$table[2,2],
    cm_afd$table[1,1],
    round(cm_afd$byClass["Recall"], 3),
    round(cm_afd$byClass["Precision"], 3),
    round(cm_afd$byClass["F1"], 3)
  )
)

kable(comparison, caption = "Synthesis of predictions by model") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
  
```
Overall, the Naive Bayes model outperforms the DFA-based approach after LSA reduction. It presents a very low number of errors on false positives and false negatives, showing an excellent capacity for discrimination between human texts and AI-generated texts.

In contrast, DFA presents a higher number of false positives indicating a less precise separation of classes probably due to a loss of information induced by dimensionality reduction.

These results therefore suggest that the full representation exploited by the Bayesian model is more suitable than the reduced space resulting from the LSA + LDA combination.

## ROC Curves and Probabilistic Analysis

Comparison of ROC curves and analysis of decision thresholds.

```{r roc-plot}
if (!require("pROC")) install.packages("pROC")
library(pROC)

roc_nb <- roc(y_test, pred_nb_prob)
roc_afd <- roc(y_test, pred_afd_prob)


plot(roc_nb, col = "#18bc9c", lwd = 2, main = "ROC Curve Comparison (NB vs DFA)")
plot(roc_afd, col = "#3498db", lwd = 2, add = TRUE)


legend("bottomright", legend = c(
    paste("Naive Bayes (AUC =", round(auc(roc_nb), 3), ")"),
    paste("DFA (AUC =", round(auc(roc_afd), 3), ")")
  ), col = c("#18bc9c", "#3498db"), lwd = 2)
```

### AUC-ROC Analysis

The **AUC (Area Under the Curve)** measures the probability that an AI text is classified with a higher probability than a human text, independently of the decision threshold.

- **AUC = 1.0**: Perfect discrimination
- **AUC = 0.5**: No better than random
- **AUC > 0.7**: Good model
- **AUC > 0.8**: Very good model

```{r auc-comparison}
# Comparative table of AUCs
auc_table <- data.frame(
  Model = c("Naive Bayes", "DFA (LSA 50 dim)"),
  AUC = c(round(auc(roc_nb), 4), round(auc(roc_afd), 4)),
  Interpretation = c(
    ifelse(auc(roc_nb) > 0.8, "Very good", ifelse(auc(roc_nb) > 0.7, "Good", "Medium")),
    ifelse(auc(roc_afd) > 0.8, "Very good", ifelse(auc(roc_afd) > 0.7, "Good", "Medium"))
  )
)

kable(auc_table, caption = "Comparison of AUC-ROCs", digits = 4) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

The ROC analysis confirms this trend: both models discriminate very well, but Naive Bayes maintains a table advantage regardless of the decision threshold.

# Conclusion

The overall results show that the approach based on n-grams and the Naive Bayes classifier offers excellent performance for AI-generated text detection. Discriminant factor analysis, although providing an interpretable visualization of class separation, remains slightly behind in terms of raw performance.

These observations confirm, in this context, that the richness of the textual representation plays a determining role in the quality of the classification.

# Future Work

Several areas for improvement can be considered:

 - **Enrichment of linguistic features**: explore finer stylometric or syntactic indicators to better capture subtle differences between human and AI-generated texts.
 - **Experimentation with hybrid or more expressive models**: test more complex architectures to evaluate if they allow an additional gain in performance compared to the Bayesian model.
 - **Adaptation to recent generative models**: given the rapid evolution of LLMs, adjust the approach to new generations of models to ensure the robustness of the system over time.