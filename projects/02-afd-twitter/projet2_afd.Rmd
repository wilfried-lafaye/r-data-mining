---
title: "Discriminant Factor Analysis"
subtitle: "Project 2: Twitter Sentiment Analysis"
author: 
  - Marie Bouetel
  - Wilfried LAFAYE
date: "ESIEE Paris | 2025 - 2026"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: yes
    highlight: tango
    code_folding: hide
    df_print: paged
---

<style type="text/css">
/* Custom CSS for a professional look */
body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    font-size: 16px;
    line-height: 1.6;
    color: #333;
    text-align: justify;
}

h1, h2, h3 {
    color: #2c3e50; /* Dark Blue */
    font-weight: 600;
}

h1 {
    margin-top: 40px;
    border-bottom: 3px solid #18bc9c; /* Teal accent from Flatly theme */
    padding-bottom: 10px;
}

h2 {
    margin-top: 30px;
    border-left: 5px solid #18bc9c;
    padding-left: 10px;
}

.title {
    text-align: center;
    font-size: 2.5em;
    font-weight: bold;
    color: #2c3e50;
    margin-bottom: 10px;
}

.subtitle {
    text-align: center;
    font-size: 1.5em;
    color: #7f8c8d;
    margin-bottom: 30px;
}

.author, .date {
    text-align: center;
    font-size: 1.2em;
    color: #34495e;
}

.main-container {
    max-width: 1000px;
    margin-left: auto;
    margin-right: auto;
}

blockquote {
    background: #f9f9f9;
    border-left: 5px solid #18bc9c;
    margin: 1.5em 10px;
    padding: 0.5em 10px;
    font-style: italic;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center")
```

# Introduction

This project is based on the **Twitter Sentiment Analysis Dataset**.

## Dataset Overview

This is an entity-level sentiment analysis dataset from Twitter. For each observation, we have a **message** (tweet) and an **entity** (topic, brand, video game, etc.). The task is to predict the sentiment expressed in the message regarding that specific entity.

The data is annotated according to the following classes:
*   **Positive**
*   **Negative**
*   **Neutral**
*   **Irrelevant**

## Study Objectives

Our analysis is broken down into several stages:

1.  **Binary classification (2 classes)**: Positive vs Negative. This is the simplest case and establishes a solid baseline.
2.  **Ternary classification (3 classes)**: Positive, Negative, Neutral (by merging Neutral and Irrelevant).
3.  **Full classification (4 classes)**: Using all classes, with a hierarchical approach to improve accuracy.

The primary metric used to evaluate our classification quality will be **Accuracy (Top-1)**.

## Data Loading and Partitioning

To ensure a rigorous evaluation, we solely use the **twitter_training.csv** file. 
**Why?** We detected that the default validation file provided was a subset of the training file, creating data leakage and skewing results.

Therefore, we perform a manual split:
*   **80%** of data for training (Word2Vec + AFD/LDA).
*   **20%** of data for final validation (generalization test).

```{r data-loading-split}
# Load necessary packages
invisible(suppressPackageStartupMessages(library(tidyverse)))
invisible(suppressPackageStartupMessages(library(tm)))

# 1. Load main dataset
col_names <- c("id", "entity", "sentiment", "text")
training_file <- "../data/twitter_training.csv"
all_data_raw <- read_csv(training_file, col_names = col_names, show_col_types = FALSE)

# 2. Cleaning and AI Integration (Gemini)
# Load AI-processed file
gemini_file <- "../data/twitter_training_gemini.csv"

if (file.exists(gemini_file)) {
  # If Gemini file exists, use it
  all_data_clean <- read_csv(gemini_file, show_col_types = FALSE) %>%
    rename(sentiment_original = original_sentiment, 
           sentiment = llm_sentiment) %>% # Use AI-corrected sentiment as reference
    distinct(id, .keep_all = TRUE) %>%
    filter(!is.na(text)) %>%
    # Surgical cleaning requested by user
    # 1. Remove tweets with less than 2 characters (e.g., ".", "[")
    filter(nchar(trimws(text)) > 1) %>% 
    # 2. Remove isolated stopwords (Noise: "The", "So", "And"..)
    filter(!str_detect(text, regex("^(the|so|and|but|or|of|to|in|on|at|by|for)$", ignore_case = TRUE)))
    
  cat("=== AI Data Loaded (Gemini) ===\n")
  cat("Source :", gemini_file, "\n")
} else {
  # Fallback to manual cleaning if AI file is missing
  all_data_clean <- all_data_raw %>%
    distinct(id, .keep_all = TRUE) %>%
    filter(!is.na(text))
  cat("=== Raw Data Loaded (Fallback) ===\n")
}

cat("Retained observations:", nrow(all_data_clean), "\n")
```

## Critical Analysis of Data Quality

Before modeling, it is essential to examine the intrinsic quality of the dataset. Our analysis revealed **4 major issues** which explain why this dataset is particularly challenging to classify rigorously.

### Issue 1: Artificial Augmentation via Paraphrasing

The dataset contains numerous repetitions of the same tweet as paraphrases. Each unique ID is associated with approximately **6 variants** of the same message:

```{r quality-paraphrase, echo=TRUE}
# Example: ID 2401 - Variations of the same tweet
paraphrase_example <- all_data_raw %>%
  filter(id == 2401) %>%
  select(id, sentiment, text)

cat("=== Paraphrase Example (ID 2401) ===\n")
cat("Number of variants:", nrow(paraphrase_example), "\n\n")
for (i in 1:min(4, nrow(paraphrase_example))) {
  cat(paste0("[", i, "] ", substr(paraphrase_example$text[i], 1, 70), "...\n"))
}
```

> **Impact**: If a random row-wise split is performed, the model sees 5 variants in training and the 6th in test. It performs *pattern matching* rather than learning the sentiment. This is why our **unique ID split** method is the only guarantee against data leakage.

### Issue 2: Semantic Deterioration (Corruption)

Some variants are semantically altered to the point of losing all meaning. Full tweets sometimes become **a single word** while retaining their original label:

```{r quality-corruption, echo=TRUE}
# Search for very short rows (less than 5 chars) with a valid label
short_rows <- all_data_raw %>%
  filter(nchar(as.character(text)) <= 5 & !is.na(sentiment)) %>%
  slice_sample(n = 8)

cat("=== Corrupted Rows Examples ===\n")
cat("(Texts under 5 characters with a sentiment label)\n\n")
for (i in 1:nrow(short_rows)) {
  cat(paste0("[", short_rows$sentiment[i], "] \"", short_rows$text[i], "\"\n"))
}
```

> **Impact**: How can an isolated word like "was" or "is" convey a "Positive" or "Negative" sentiment? These lines are **pure noise** that our model must ignore.

### Issue 3: Training / Validation Overlap

The default `twitter_validation.csv` file provided is **NOT independent** from the training file:

```{r quality-leakage, echo=TRUE}
# Load provided validation file
validation_file <- "../data/twitter_validation.csv"
if(file.exists(validation_file)) {
  valid_raw <- read_csv(validation_file, col_names = col_names, show_col_types = FALSE)
  
  # Compare by text (cleaned to lowercase)
  clean_text <- function(x) str_to_lower(str_trim(as.character(x)))
  train_texts <- unique(clean_text(all_data_raw$text))
  valid_texts <- unique(clean_text(valid_raw$text))
  common_texts <- intersect(train_texts, valid_texts)
  
  cat("=== Train/Validation Overlap Analysis ===\n")
  cat("Unique phrases in Training   :", length(train_texts), "\n")
  cat("Unique phrases in Validation :", length(valid_texts), "\n")
  cat("COMMON phrases (identical)   :", length(common_texts), "\n")
  cat("Leakage Percentage           :", round(length(common_texts)/length(valid_texts)*100, 1), "%\n")
}
```

> **Impact**: Over **50% of validation phrases** are already present in training! Models using this naive split achieve ~98% accuracy, but it's **memorization, not generalization**.

### Issue 4: Hallucinations within Variants

Paraphrasing algorithms sometimes **hallucinate entities** or completely change the tweet's subject:

```{r quality-hallucination, echo=TRUE}
# Example: ID 2417 where "borderlands" becomes other subjects
hallucination_example <- all_data_raw %>%
  filter(id == 2417) %>%
  select(text)

cat("=== Hallucination Example (ID 2417) ===\n")
cat("Original subject: Borderlands\n\n")
for (i in 1:min(4, nrow(hallucination_example))) {
  snippet <- substr(hallucination_example$text[i], 1, 90)
  cat(paste0("[", i, "] ", snippet, "...\n"))
}
```

> **Conclusion**: This dataset is a typical academic benchmark **stress-test**. Achieving ~48% accuracy on 4 classes with a clean split is an **honest and scientifically valid** result, unlike the 98% obtained through paraphrase memorization.

```{r continue-prep}
# 3. Preparation for main study (2 classes)
training_data_full <- all_data_clean %>%
  filter(sentiment %in% c("Positive", "Negative")) %>%
  mutate(sentiment = factor(sentiment, levels = c("Negative", "Positive")))

# Split by Unique ID for 2-class study
unique_ids_2c <- unique(training_data_full$id)
set.seed(42)
train_ids_2c <- sample(unique_ids_2c, size = 0.8 * length(unique_ids_2c))

training_data <- training_data_full %>% filter(id %in% train_ids_2c)
validation_data <- training_data_full %>% filter(!(id %in% train_ids_2c))

cat("\n=== Partitioning (2 classes) ===\n")
cat("Training   :", nrow(training_data), " (80%)\n")
cat("Validation :", nrow(validation_data), " (20%)\n")
```

# Methodology

## Data Preprocessing

Cleaning textual data is a crucial step in sentiment analysis. The goal is to **reduce noise** while **retaining meaningful words** that discriminate sentiment classes.

### Justification of applied transformations

We apply the following transformations, each serving a specific role:

1. **Remove URLs** (`https://...`, `www...`)
2. **Remove mentions** (`@user`)
3. **Remove hashtags** (`#topic`)
4. **Remove special characters and punctuation**
5. **Convert to lowercase**
6. **Remove stopwords**

```{r text-cleaning}
# Tweet cleaning function (Vectorized)
clean_texts <- function(texts) {
  # 1. URLs
  texts <- gsub("https?://\\S+|www\\.\\S+", "", texts)
  # 2. Mentions
  texts <- gsub("@\\w+", "", texts)
  # 3. Hashtags
  texts <- gsub("#\\w+", "", texts)
  # 4. Special characters
  texts <- gsub("[^a-zA-Z ]", "", texts)
  # 5. Lowercase
  texts <- tolower(texts)
  # 6. Multiple spaces
  texts <- gsub(" +", " ", texts)
  # 7. Trim
  texts <- trimws(texts)
  return(texts)
}

# Stopwords removal
# Note: We intentionally keep stopwords as they help identify "Neutral" tweets
# (linking phrases like "the", "is", "of" characterize this style)
stopwords_en <- stopwords("en")
remove_stopwords_vec <- function(texts) {
  return(texts)  # Stopwords kept intentionally
}

# Global cleaning application on all_data_clean for reuse
cat("=== Global Data Cleaning (WITH Stopwords) ===\n")
all_data_clean$text_clean <- clean_texts(all_data_clean$text)
all_data_clean$text_clean <- remove_stopwords_vec(all_data_clean$text_clean)

# Apply to subsets (via join or direct update)
training_data$text_clean <- all_data_clean$text_clean[match(training_data$id, all_data_clean$id)]
validation_data$text_clean <- all_data_clean$text_clean[match(validation_data$id, all_data_clean$id)]

cat("Data cleaning completed!\n")
```

```{r cleaning-results}
# Display before/after cleaning
cat("=== Cleaning Examples ===\n\n")

set.seed(42)
sample_idx <- sample(1:nrow(training_data), 5)

for (i in sample_idx) {
  cat("BEFORE:", substr(training_data$text[i], 1, 80), "...\n")
  cat("AFTER :", substr(training_data$text_clean[i], 1, 80), "\n\n")
}

# Remove rows with empty text after cleaning
training_data <- training_data %>% filter(text_clean != "")
validation_data <- validation_data %>% filter(text_clean != "")
```

```{r sentiment-encoding}
# Numeric encoding of sentiment labels
cat("=== Sentiment Numeric Encoding ===\n\n")
sentiment_levels <- levels(training_data$sentiment)
print(sentiment_levels)

training_data <- training_data %>% mutate(sentiment_num = as.numeric(sentiment))
validation_data <- validation_data %>% mutate(sentiment_num = as.numeric(sentiment))
```

# Feature Engineering: Word Embeddings (Word2Vec)

We move from a **sparse** representation (TF-IDF) to a **dense** representation (Word Embeddings).
The idea is to represent each word with a vector of real numbers (e.g., size 100) capturing semantic meaning.
Then, to represent a tweet, we compute the **mean of vectors** of the words it contains. The advantages are multiple:
- Semantics captured ("good" and "great" have close vectors).
- Automatic dimensionality reduction (100 dimensions vs 5000+ with TF-IDF).
- No sparsity issues.

## Word2Vec Model Training

```{r word2vec-training}
# Install and load word2vec package
if (!require("word2vec")) install.packages("word2vec", repos = "https://cloud.r-project.org")
library(word2vec)

# 1. Bigram Detection and Application (n-grams)
# This step learns vectors for expressions like "not_bad"
cat("=== Detecting Frequent Bigrams ===\n")

find_frequent_bigrams <- function(texts, top_n = 200) {
  # Simple tokenization to count pairs
  words_list <- strsplit(texts, " ")
  bigrams <- unlist(lapply(words_list, function(w) {
    if(length(w) < 2) return(NULL)
    paste(w[-length(w)], w[-1], sep = "_")
  }))
  # Counting and selecting best
  sort(table(bigrams), decreasing = TRUE)[1:top_n]
}

# Extract most frequent bigrams from training set
top_bigrams <- find_frequent_bigrams(training_data$text_clean, top_n = 200)
bigram_list <- names(top_bigrams)

# Function to merge words forming a bigram (e.g., "not bad" -> "not_bad")
apply_bigrams <- function(texts, bigrams) {
  for(bg in bigrams) {
    target <- gsub("_", " ", bg)
    texts <- gsub(paste0("\\b", target, "\\b"), bg, texts)
  }
  return(texts)
}

cat("Transforming texts with bigrams...\n")
training_data$text_bigrams <- apply_bigrams(training_data$text_clean, bigram_list)
validation_data$text_bigrams <- apply_bigrams(validation_data$text_clean, bigram_list)

# 2. Train Word2Vec Model (Skip-Gram with Bigrams)
cat("=== Training Word2Vec Model (Bigrams) ===\n")

# Tokenization into list for word2vec (more robust)
train_tokens_list <- strsplit(training_data$text_bigrams, " ")
# Safeguard: remove empty elements or NULL
train_tokens_list <- train_tokens_list[sapply(train_tokens_list, length) > 0]

set.seed(42)
w2v_model <- word2vec(x = train_tokens_list, type = "skip-gram", dim = 500, window = 7, min_count = 5, iter = 30, threads = 1)

cat("Word2Vec model trained successfully!\n")
vocab_size <- nrow(as.matrix(w2v_model))
cat("Learned vocabulary size:", vocab_size, "words (including bigrams)\n")
```

## Vectorization of Tweets (Document Embeddings)

For each tweet, we construct a vector by averaging the vectors of its words.

```{r doc-vectorization}
# Function to vectorize a set of texts
get_document_vectors <- function(model, texts) {
  # Explicit tokenization into list to avoid doc2vec() errors
  tokens_list <- strsplit(texts, " ")
  
  # doc2vec computes the mean of embeddings
  doc_vectors <- doc2vec(model, tokens_list)
  
  # Replacing NA with zeros
  doc_vectors[is.na(doc_vectors)] <- 0
  
  return(as.data.frame(doc_vectors))
}

cat("=== Vectorizing Tweets (Bigrams) ===\n")

# Vectorization with transformed texts
train_vectors <- get_document_vectors(w2v_model, training_data$text_bigrams)
valid_vectors <- get_document_vectors(w2v_model, validation_data$text_bigrams)

cat("Vectors generated successfully.\n")
```

```{r prepare-afd-data}
# Final data preparation for AFD/LDA
# Combining dense vectors with sentiment labels

train_afd <- data.frame(
  sentiment = training_data$sentiment,
  train_vectors
)

valid_afd <- data.frame(
  sentiment = validation_data$sentiment,
  valid_vectors
)
```

# AFD Implementation

## Mathematical Foundations

**Discriminant Factor Analysis** (AFD/LDA) is used here on dense vectors to find the axis that best separates Positive and Negative sentiments.
Unlike the TF-IDF approach where AFD suffered from the large number of variables, here it works on 100 dense and relevant variables, which is ideal.

## Application in R with MASS

```{r lda-training}
library(MASS)

# Variance check (rarely zero with word2vec but safe to do)
var_cols <- apply(train_afd[, -1], 2, var) > 1e-10
train_afd_clean <- train_afd[, c(TRUE, var_cols)]

cat("=== Training LDA Model ===\n")
cat("Variables used:", ncol(train_afd_clean) - 1, "\n")
cat("Observations:", nrow(train_afd_clean), "\n")

# Training
lda_model <- lda(sentiment ~ ., data = train_afd_clean)
cat("LDA Model trained successfully!\n")
```

```{r lda-projection}
# Projecting data onto discriminant axes

# Training
train_projected <- predict(lda_model, train_afd_clean)
train_lda_scores <- as.data.frame(train_projected$x)
train_lda_scores$sentiment <- train_afd_clean$sentiment

# Validation (same columns)
valid_afd_clean <- valid_afd[, names(train_afd_clean)]
valid_projected <- predict(lda_model, valid_afd_clean)
valid_lda_scores <- as.data.frame(valid_projected$x)
valid_lda_scores$sentiment <- valid_afd_clean$sentiment

cat("=== Projected Data ===\n")
head(train_lda_scores)
```

# Results

## Visualization

### Distribution on Discriminant Axis LD1

```{r viz-ld1, fig.width=10, fig.height=6}
library(ggplot2)

sentiment_colors <- c("Negative" = "#e74c3c", "Positive" = "#2ecc71")

set.seed(42)
sample_size <- min(5000, nrow(train_lda_scores))
train_sample <- train_lda_scores[sample(1:nrow(train_lda_scores), sample_size), ]

ggplot(train_sample, aes(x = LD1, fill = sentiment)) +
  geom_density(alpha = 0.6) +
  scale_fill_manual(values = sentiment_colors) +
  labs(
    title = "Sentiment Separation on LD1 Axis (Word2Vec + AFD)",
    subtitle = "Dense vectors generally allow for cleaner separation",
    x = "LD1 Score",
    y = "Density"
  ) +
  theme_minimal(base_size = 14)
```

## Classification Performance

To detect overfitting, we compare performance on training and validation data.

```{r classification-metrics}
# --- Performance on Training (for checking overfitting) ---
pred_train <- train_projected$class
actual_train <- train_afd_clean$sentiment
acc_train <- sum(pred_train == actual_train) / length(actual_train)

# --- Performance on Validation ---
predicted_classes <- valid_projected$class
actual_classes <- valid_afd_clean$sentiment
acc_valid <- sum(predicted_classes == actual_classes) / length(actual_classes)

cat("=== Accuracy Comparison (Overfitting Check) ===\n")
cat(sprintf("Training Accuracy   : %.2f%%\n", acc_train * 100))
cat(sprintf("Validation Accuracy : %.2f%%\n", acc_valid * 100))

gap <- (acc_train - acc_valid) * 100
cat(sprintf("Gap                 : %.2f points\n", gap))

if(gap > 10) {
  cat("/!\\ WARNING : Significant gap (>10%), high risk of overfitting.\n")
} else if (gap > 5) {
  cat("Moderate warning: Notable gap between 5% and 10%.\n")
} else {
  cat("Good generalization: Small gap (<5%).\n")
}

# Confusion Matrix (Validation)
cat("\n=== Confusion Matrix (Validation) ===\n")
table(Actual = actual_classes, Predicted = predicted_classes)
```

## Key Methodological Points

Our approach relies on two essential methodological choices:

1.  **Unique ID Split**: Paraphrased tweets share the same ID. A classic random split would lead to data leakage (the model would see paraphrases in training and their original in validation). Splitting by ID ensures dataset independence.

2.  **Keeping Stopwords**: Unlike the classic approach, we keep stopwords. They are essential for identifying "Neutral" tweets (linking phrases like "the", "is", "of").

> [!TIP]
> Combining these two choices allows us to obtain an **honest** evaluation of the model's performance, with a Training/Validation gap of less than 1%.

# Comparative Study: Impact of Number of Classes

We reuse the trained Word2Vec model to vectorize and classify datasets with 3 and 4 classes (using deduplicated data).

```{r comparative-study, fig.width=10, fig.height=6}
cat("=== Comparative Approaches Study ===\n")
accuracy_2_classes <- acc_valid

# 1. 4-class Data Prep (Positive, Negative, Neutral, Irrelevant)
all_data_4c <- all_data_clean %>% 
  filter(sentiment %in% c("Positive", "Negative", "Neutral", "Irrelevant")) %>%
  filter(text_clean != "")

# Unique ID Split (Repro)
unique_ids_4c <- unique(all_data_4c$id)
set.seed(42)
train_ids_4c <- sample(unique_ids_4c, size = 0.8 * length(unique_ids_4c))

training_4c <- all_data_4c %>% filter(id %in% train_ids_4c)
validation_4c <- all_data_4c %>% filter(!(id %in% train_ids_4c))

# Vectorization (done once for 3/4/H models)
train_vec_4c <- get_document_vectors(w2v_model, training_4c$text_clean)
valid_vec_4c <- get_document_vectors(w2v_model, validation_4c$text_clean)

# --- 4 Classes (Direct Approach) ---
train_df_4c <- data.frame(sentiment = factor(training_4c$sentiment), train_vec_4c)
valid_df_4c <- data.frame(sentiment = factor(validation_4c$sentiment), valid_vec_4c)

lda_4c <- lda(sentiment ~ ., data = train_df_4c)
pred_4c <- predict(lda_4c, valid_df_4c)$class
accuracy_4_classes <- mean(pred_4c == validation_4c$sentiment)

# --- Overfitting Check (4 classes direct) ---
pred_4c_train <- predict(lda_4c, train_df_4c)$class
acc_4c_train <- mean(pred_4c_train == training_4c$sentiment)
cat("\n=== Overfitting Check (4 classes - Direct) ===\n")
cat(sprintf("Training Accuracy   : %.2f%%\n", acc_4c_train * 100))
cat(sprintf("Validation Accuracy : %.2f%%\n", accuracy_4_classes * 100))
cat(sprintf("Gap                 : %.2f points\n", (acc_4c_train - accuracy_4_classes) * 100))

# --- 3 Classes (Direct Approach) ---
# Merge Neutral/Irrelevant into "Neutral"
training_3c <- training_4c %>% mutate(sentiment_3c = ifelse(sentiment == "Irrelevant", "Neutral", sentiment))
validation_3c <- validation_4c %>% mutate(sentiment_3c = ifelse(sentiment == "Irrelevant", "Neutral", sentiment))

train_df_3c <- data.frame(sentiment = factor(training_3c$sentiment_3c), train_vec_4c)
valid_df_3c <- data.frame(sentiment = factor(validation_3c$sentiment_3c), valid_vec_4c)

lda_3c <- lda(sentiment ~ ., data = train_df_3c)
pred_3c <- predict(lda_3c, valid_df_3c)$class
accuracy_3_classes <- mean(pred_3c == validation_3c$sentiment_3c)
```

# --- 4 Classes (Hierarchical Approach) ---

The hierarchical approach aims to resolve the confusion between polar classes (Positive/Negative) and informative classes (Neutral/Irrelevant) by breaking the problem down into two logical steps:

1.  **Level 1: Distinguish Tone (Subjective vs Objective)**
    *   **Subjective**: Tweets expressing an emotion or opinion (labels *Positive* and *Negative*).
    *   **Objective**: Factual linking or irrelevant tweets (labels *Neutral* and *Irrelevant*).
    
2.  **Level 2: Refine Prediction**
    *   If the tweet is deemed **Subjective**, we choose between *Positive* and *Negative*.
    *   If the tweet is deemed **Objective**, we choose between *Neutral* and *Irrelevant*.

> [!NOTE]
> **Why this approach?** In a latent space (AFD), "Positive" and "Negative" tweets often possess semantic structures sharply distinct from "Neutral" tweets. By first isolating "subjectivity", we allow second-level classifiers to be more specialized and accurate.

```{r hierarchical-classification}
# Prepare hierarchical labels
# Type: Subjective (Pos/Neg) vs Objective (Neutral/Irrelevant)
training_4c <- training_4c %>% mutate(
  is_subjective = ifelse(sentiment %in% c("Positive", "Negative"), "Subjective", "Objective"),
  is_subjective = factor(is_subjective, levels = c("Objective", "Subjective"))
)
validation_4c <- validation_4c %>% mutate(
  is_subjective = ifelse(sentiment %in% c("Positive", "Negative"), "Subjective", "Objective"),
  is_subjective = factor(is_subjective, levels = c("Objective", "Subjective"))
)

# Level 1 Model: Subjective vs Objective
cat("\n--- Level 1: Subjective vs Objective ---\n")
train_df_lvl1 <- data.frame(is_subjective = training_4c$is_subjective, train_vec_4c)
lda_lvl1 <- lda(is_subjective ~ ., data = train_df_lvl1)
pred_lvl1 <- predict(lda_lvl1, valid_df_4c[, -1])$class
acc_lvl1 <- mean(pred_lvl1 == validation_4c$is_subjective)
cat(sprintf("Level 1 Accuracy   : %.2f%%\n", acc_lvl1 * 100))

# Level 2a Model: Positive vs Negative (Only on subjective data)
train_subj_idx <- which(training_4c$is_subjective == "Subjective")
train_df_subj <- data.frame(sentiment = factor(training_4c$sentiment[train_subj_idx], levels = c("Negative", "Positive")), 
                               train_vec_4c[train_subj_idx, ])
lda_subj <- lda(sentiment ~ ., data = train_df_subj)

# Level 2b Model: Neutral vs Irrelevant (Only on objective data)
train_obj_idx <- which(training_4c$is_subjective == "Objective")
train_df_obj <- data.frame(sentiment = factor(training_4c$sentiment[train_obj_idx], levels = c("Neutral", "Irrelevant")), 
                               train_vec_4c[train_obj_idx, ])
lda_obj <- lda(sentiment ~ ., data = train_df_obj)

# --- HIERARCHICAL INFERENCE ---
# We use level 1 to route each validation tweet
final_h_preds <- character(nrow(validation_4c))
for(i in 1:nrow(validation_4c)) {
  current_vec <- valid_vec_4c[i, , drop = FALSE]
  if(pred_lvl1[i] == "Subjective") {
    final_h_preds[i] <- as.character(predict(lda_subj, current_vec)$class)
  } else {
    final_h_preds[i] <- as.character(predict(lda_obj, current_vec)$class)
  }
}

accuracy_hierarchical <- mean(final_h_preds == validation_4c$sentiment)
cat(sprintf("Hierarchical Accuracy (4 classes) : %.2f%%\n", accuracy_hierarchical * 100))

# --- Overfitting Check (4 classes hierarchical) ---
# Prediction on training using the same hierarchical logic
pred_lvl1_train <- predict(lda_lvl1, train_df_lvl1[, -1])$class
final_h_preds_train <- character(nrow(training_4c))
for(i in 1:nrow(training_4c)) {
  current_vec <- train_vec_4c[i, , drop = FALSE]
  if(pred_lvl1_train[i] == "Subjective") {
    final_h_preds_train[i] <- as.character(predict(lda_subj, current_vec)$class)
  } else {
    final_h_preds_train[i] <- as.character(predict(lda_obj, current_vec)$class)
  }
}
acc_h_train <- mean(final_h_preds_train == training_4c$sentiment)

cat("\n=== Overfitting Check (4 classes - Hierarchical) ===\n")
cat(sprintf("Training Accuracy   : %.2f%%\n", acc_h_train * 100))
cat(sprintf("Validation Accuracy : %.2f%%\n", accuracy_hierarchical * 100))
cat(sprintf("Gap                 : %.2f points\n", (acc_h_train - accuracy_hierarchical) * 100))

# Hierarchical Confusion Matrix
cat("\n=== Confusion Matrix (Hierarchical) ===\n")
table(Actual = validation_4c$sentiment, Predicted = final_h_preds)

# --- Final Comparison ---
comparison_df <- data.frame(
  Approach = factor(c("2 classes", "3 classes", "4 classes (Direct)", "4 classes (Hierarchical)"),
                   levels = c("2 classes", "3 classes", "4 classes (Direct)", "4 classes (Hierarchical)")),
  Accuracy = c(accuracy_2_classes, accuracy_3_classes, accuracy_4_classes, accuracy_hierarchical) * 100
)

ggplot(comparison_df, aes(x = Approach, y = Accuracy, fill = Approach)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = sprintf("%.1f%%", Accuracy)), vjust = -0.5) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Classification Approaches Comparison", 
       subtitle = "Evaluation of hierarchy vs direct classification")
```

# Conclusion

## Results Synthesis

This study explored several approaches for sentiment analysis on Twitter using Discriminant Factor Analysis (AFD).

```{r results-synthesis, echo=FALSE}
# Dynamic comparison table (avoids hardcoded values)
h_obs <- if (accuracy_hierarchical > accuracy_4_classes) {
  "Improvement through two-level routing"
} else {
  "Routing errors at Level 1 offset Level 2 gains"
}

synthesis_df <- data.frame(
  Approach = c("2 classes (Pos/Neg)", 
               "3 classes (Pos/Neg/Neutral)", 
               "4 classes (Direct)", 
               "4 classes (Hierarchical)"),
  Accuracy = sprintf("%.1f%%", c(accuracy_2_classes, accuracy_3_classes, 
                                  accuracy_4_classes, accuracy_hierarchical) * 100),
  Observation = c("Solid baseline, clean separation on LD1",
                   "Difficulty in isolating neutral tweets",
                   "Significant Neutral/Irrelevant confusion",
                   h_obs)
)
knitr::kable(synthesis_df, align = "lcc")
```

## Methodological Contributions

1.  **Data Quality**: We demonstrated the importance of unit ID splitting to avoid data leakage caused by paraphrasing.

2.  **Optimized Word2Vec**: Utilizing bigrams and high dimensionality (500) improves the capture of semantic nuances.

3.  **Hierarchical Classification**: Decomposing the problem into two levels (Subjective/Objective then fine sentiment) is an interesting strategy but does not guarantee improvement when Level 1 separation is difficult for linear models.

## Limitations & Perspectives

- Extremely short tweets (< 5 words) remain difficult to classify.
- The hierarchical approach could be improved with more complex models (SVM, Random Forest) at Level 2.
- Integrating an LLM (such as Gemini) to clean labels significantly enhances training data quality.
